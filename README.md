# Wikipedia Tabular Data Scraper - Module 2

The Wikipedia Tabular Data Scraper - Module 2 repository provides a Python-based solution for extracting tabular data from Wikipedia pages and saving it as CSV files. This module enhances the functionality by offering additional features such as PostgreSQL integration for storing the scraped data.

## Features

- Extracts tabular data from Wikipedia pages provided as URL by users.
- Saves the extracted data as CSV files.
- Integrates with PostgreSQL to store the scraped data.

## Requirements

- `Python 3.x`
- `requests`
- `beautifulsoup4`
- `pandas`
- `streamlit`
- `psycopg2`



## Running Locally

1. Clone the repository:
    ```bash
    git clone https://github.com/MekhzZ/Wikipedia_Tabular_Data_Scraper_-Module_2-.git
    cd Wikipedia_Tabular_Data_Scraper_-Module_2-
    ```

2. Create a virtual environment and activate it:
    ```bash
    python3 -m venv env
    source env/bin/activate   # On Windows use `env\Scripts\activate`
    ```

3. Install the required packages:
    ```bash
    pip install -r requirements.txt
    ```

4. Run the Streamlit application:
    ```bash
    streamlit run main.py
    ```

5. Open your browser and navigate to `http://localhost:8501` to access the application.

## Conclusion
The Wikipedia Tabular Data Scraper - Module 2 is a powerful tool for extracting and converting Wikipedia's tabular data into CSV format, and also supports storing data in PostgreSQL. Whether you are a researcher, data analyst, or developer, this module will save you time and effort in obtaining and managing structured data from Wikipedia.

## Documentation
feel free to go through below link for docs:
- https://docs.google.com/document/d/1kuoeFyeuknALGBJYXNImkSEoVQ1dh-btPtm8R2J27zM/edit?usp=sharing

## Contribution
Feel free to customize the content further based on your repository's specifics.

## Contact
- LinkedIn: [Mekhma Tamang](https://www.linkedin.com/in/mekhma-tamang/)
